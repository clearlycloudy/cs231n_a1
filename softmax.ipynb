{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "pdf-title"
    ]
   },
   "source": [
    "# Softmax exercise\n",
    "\n",
    "*Complete and hand in this completed worksheet (including its outputs and any supporting code outside of the worksheet) with your assignment submission. For more details see the [assignments page](http://vision.stanford.edu/teaching/cs231n/assignments.html) on the course website.*\n",
    "\n",
    "This exercise is analogous to the SVM exercise. You will:\n",
    "\n",
    "- implement a fully-vectorized **loss function** for the Softmax classifier\n",
    "- implement the fully-vectorized expression for its **analytic gradient**\n",
    "- **check your implementation** with numerical gradient\n",
    "- use a validation set to **tune the learning rate and regularization** strength\n",
    "- **optimize** the loss function with **SGD**\n",
    "- **visualize** the final learned weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from cs231n.data_utils import load_CIFAR10\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading extenrnal modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000, 3073)\n",
      "Train labels shape:  (49000,)\n",
      "Validation data shape:  (1000, 3073)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (1000, 3073)\n",
      "Test labels shape:  (1000,)\n",
      "dev data shape:  (500, 3073)\n",
      "dev labels shape:  (500,)\n"
     ]
    }
   ],
   "source": [
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000, num_dev=500):\n",
    "    \"\"\"\n",
    "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "    it for the linear classifier. These are the same steps as we used for the\n",
    "    SVM, but condensed to a single function.  \n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 data\n",
    "    cifar10_dir = 'cs231n/datasets/cifar-10-batches-py'\n",
    "    \n",
    "    # Cleaning up variables to prevent loading data multiple times (which may cause memory issue)\n",
    "    try:\n",
    "       del X_train, y_train\n",
    "       del X_test, y_test\n",
    "       print('Clear previously loaded data.')\n",
    "    except:\n",
    "       pass\n",
    "\n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "    \n",
    "    # subsample the data\n",
    "    mask = list(range(num_training, num_training + num_validation))\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = list(range(num_training))\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = list(range(num_test))\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "    mask = np.random.choice(num_training, num_dev, replace=False)\n",
    "    X_dev = X_train[mask]\n",
    "    y_dev = y_train[mask]\n",
    "    \n",
    "    # Preprocessing: reshape the image data into rows\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "    X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "    X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))\n",
    "    \n",
    "    # Normalize the data: subtract the mean image\n",
    "    mean_image = np.mean(X_train, axis = 0)\n",
    "    X_train -= mean_image\n",
    "    X_val -= mean_image\n",
    "    X_test -= mean_image\n",
    "    X_dev -= mean_image\n",
    "    \n",
    "    # add bias dimension and transform into columns\n",
    "    X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "    X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "    X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "    X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev\n",
    "\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev = get_CIFAR10_data()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)\n",
    "print('dev data shape: ', X_dev.shape)\n",
    "print('dev labels shape: ', y_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Classifier\n",
    "\n",
    "Your code for this section will all be written inside **cs231n/classifiers/softmax.py**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.364604\n",
      "sanity check: 2.302585\n"
     ]
    }
   ],
   "source": [
    "# First implement the naive softmax loss function with nested loops.\n",
    "# Open the file cs231n/classifiers/softmax.py and implement the\n",
    "# softmax_loss_naive function.\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_naive\n",
    "import time\n",
    "\n",
    "# Generate a random softmax weight matrix and use it to compute the loss.\n",
    "W = np.random.randn(3073, 10) * 0.0001\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As a rough sanity check, our loss should be something close to -log(0.1).\n",
    "print('loss: %f' % loss)\n",
    "print('sanity check: %f' % (-np.log(0.1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "pdf-inline"
    ]
   },
   "source": [
    "**Inline Question 1**\n",
    "\n",
    "Why do we expect our loss to be close to -log(0.1)? Explain briefly.**\n",
    "\n",
    "$\\color{blue}{\\textit Your Answer:}$\n",
    "Weights are randomly initialized hence uniform.\n",
    "1/10 probability paritioning for each class, then\n",
    "loss = -log(1/10) = log(10) = 2.3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical: 2.176305 analytic: 2.176305, relative error: 1.761543e-08\n",
      "numerical: -0.177533 analytic: -0.177533, relative error: 4.554767e-07\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-a31f33fafa53>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcs231n\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_check\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgrad_check_sparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msoftmax_loss_naive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mgrad_numerical\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_check_sparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# similar to SVM case, do another gradient check with regularization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/repo/cs231n_a1/cs231n/gradient_check.py\u001b[0m in \u001b[0;36mgrad_check_sparse\u001b[0;34m(f, x, analytic_grad, num_checks, h)\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0mfxph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# evaluate f(x + h)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moldval\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mh\u001b[0m \u001b[0;31m# increment by h\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mfxmh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# evaluate f(x - h)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moldval\u001b[0m \u001b[0;31m# reset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-a31f33fafa53>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(w)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# The numeric gradient should be close to the analytic gradient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcs231n\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_check\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgrad_check_sparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msoftmax_loss_naive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mgrad_numerical\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_check_sparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/repo/cs231n_a1/cs231n/classifiers/softmax.py\u001b[0m in \u001b[0;36msoftmax_loss_naive\u001b[0;34m(W, X, y, reg)\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_feature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#inner product\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m                 \u001b[0mh\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#d_4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m             \u001b[0mdenom\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#d_3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Complete the implementation of softmax_loss_naive and implement a (naive)\n",
    "# version of the gradient that uses nested loops.\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As we did for the SVM, use numeric gradient checking as a debugging tool.\n",
    "# The numeric gradient should be close to the analytic gradient.\n",
    "from cs231n.gradient_check import grad_check_sparse\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 0.0)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)\n",
    "\n",
    "# similar to SVM case, do another gradient check with regularization\n",
    "print(\"gradient regularization check\")\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 5e1)\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 5e1)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive loss: 2.364599e+00 computed in 32.426111s\n",
      "vectorized loss: 2.364599e+00 computed in 0.084004s\n",
      "Loss difference: 0.000000\n",
      "Gradient difference: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Now that we have a naive implementation of the softmax loss function and its gradient,\n",
    "# implement a vectorized version in softmax_loss_vectorized.\n",
    "# The two versions should compute the same results, but the vectorized version should be\n",
    "# much faster.\n",
    "tic = time.time()\n",
    "loss_naive, grad_naive = softmax_loss_naive(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('naive loss: %e computed in %fs' % (loss_naive, toc - tic))\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_vectorized\n",
    "tic = time.time()\n",
    "loss_vectorized, grad_vectorized = softmax_loss_vectorized(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('vectorized loss: %e computed in %fs' % (loss_vectorized, toc - tic))\n",
    "\n",
    "# As we did for the SVM, we use the Frobenius norm to compare the two versions\n",
    "# of the gradient.\n",
    "grad_difference = np.linalg.norm(grad_naive - grad_vectorized, ord='fro')\n",
    "print('Loss difference: %f' % np.abs(loss_naive - loss_vectorized))\n",
    "print('Gradient difference: %f' % grad_difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "code"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l,r: 1e-07 25000.0\n",
      "iteration 0 / 1500: loss 767.269389\n",
      "iteration 100 / 1500: loss 281.325097\n",
      "iteration 200 / 1500: loss 104.303464\n",
      "iteration 300 / 1500: loss 39.520891\n",
      "iteration 400 / 1500: loss 15.690025\n",
      "iteration 500 / 1500: loss 7.103176\n",
      "iteration 600 / 1500: loss 3.864405\n",
      "iteration 700 / 1500: loss 2.785862\n",
      "iteration 800 / 1500: loss 2.301653\n",
      "iteration 900 / 1500: loss 2.138629\n",
      "iteration 1000 / 1500: loss 2.100193\n",
      "iteration 1100 / 1500: loss 2.083275\n",
      "iteration 1200 / 1500: loss 2.082566\n",
      "iteration 1300 / 1500: loss 2.104491\n",
      "iteration 1400 / 1500: loss 2.080492\n",
      "iteration 0 / 1500: loss 781.994759\n",
      "iteration 100 / 1500: loss 286.965102\n",
      "iteration 200 / 1500: loss 106.350002\n",
      "iteration 300 / 1500: loss 40.224470\n",
      "iteration 400 / 1500: loss 15.996663\n",
      "iteration 500 / 1500: loss 7.230168\n",
      "iteration 600 / 1500: loss 3.897565\n",
      "iteration 700 / 1500: loss 2.796705\n",
      "iteration 800 / 1500: loss 2.377315\n",
      "iteration 900 / 1500: loss 2.144059\n",
      "iteration 1000 / 1500: loss 2.133248\n",
      "iteration 1100 / 1500: loss 2.091542\n",
      "iteration 1200 / 1500: loss 2.095258\n",
      "iteration 1300 / 1500: loss 2.125375\n",
      "iteration 1400 / 1500: loss 2.091858\n",
      "iteration 0 / 1500: loss 776.659682\n",
      "iteration 100 / 1500: loss 285.270605\n",
      "iteration 200 / 1500: loss 105.588128\n",
      "iteration 300 / 1500: loss 39.998393\n",
      "iteration 400 / 1500: loss 15.875243\n",
      "iteration 500 / 1500: loss 7.175962\n",
      "iteration 600 / 1500: loss 3.988012\n",
      "iteration 700 / 1500: loss 2.783312\n",
      "iteration 800 / 1500: loss 2.338355\n",
      "iteration 900 / 1500: loss 2.206541\n",
      "iteration 1000 / 1500: loss 2.169134\n",
      "iteration 1100 / 1500: loss 2.088615\n",
      "iteration 1200 / 1500: loss 2.121484\n",
      "iteration 1300 / 1500: loss 2.028937\n",
      "iteration 1400 / 1500: loss 2.145715\n",
      "iteration 0 / 1500: loss 775.123582\n",
      "iteration 100 / 1500: loss 284.545244\n",
      "iteration 200 / 1500: loss 105.399977\n",
      "iteration 300 / 1500: loss 39.833928\n",
      "iteration 400 / 1500: loss 15.915824\n",
      "iteration 500 / 1500: loss 7.181725\n",
      "iteration 600 / 1500: loss 3.946940\n",
      "iteration 700 / 1500: loss 2.694419\n",
      "iteration 800 / 1500: loss 2.351193\n",
      "iteration 900 / 1500: loss 2.155946\n",
      "iteration 1000 / 1500: loss 2.135766\n",
      "iteration 1100 / 1500: loss 2.054672\n",
      "iteration 1200 / 1500: loss 2.065127\n",
      "iteration 1300 / 1500: loss 2.077617\n",
      "iteration 1400 / 1500: loss 2.097452\n",
      "iteration 0 / 1500: loss 786.693830\n",
      "iteration 100 / 1500: loss 288.808860\n",
      "iteration 200 / 1500: loss 107.008656\n",
      "iteration 300 / 1500: loss 40.394197\n",
      "iteration 400 / 1500: loss 16.147224\n",
      "iteration 500 / 1500: loss 7.224413\n",
      "iteration 600 / 1500: loss 4.053681\n",
      "iteration 700 / 1500: loss 2.734721\n",
      "iteration 800 / 1500: loss 2.284205\n",
      "iteration 900 / 1500: loss 2.145678\n",
      "iteration 1000 / 1500: loss 2.132901\n",
      "iteration 1100 / 1500: loss 2.099544\n",
      "iteration 1200 / 1500: loss 2.102995\n",
      "iteration 1300 / 1500: loss 2.041487\n",
      "iteration 1400 / 1500: loss 2.052082\n",
      "[(0.3311224489795918, 0.33438775510204083), (0.3348469387755102, 0.31979591836734694), (0.3261734693877551, 0.3336734693877551), (0.3302040816326531, 0.3323469387755102), (0.331734693877551, 0.3240816326530612)]\n",
      "validation accuracy: 0.328857\n",
      "l,r: 1e-07 35000.0\n",
      "iteration 0 / 1500: loss 1090.804158\n",
      "iteration 100 / 1500: loss 268.274664\n",
      "iteration 200 / 1500: loss 67.245304\n",
      "iteration 300 / 1500: loss 18.083369\n",
      "iteration 400 / 1500: loss 5.991182\n",
      "iteration 500 / 1500: loss 3.040272\n",
      "iteration 600 / 1500: loss 2.355705\n",
      "iteration 700 / 1500: loss 2.184489\n",
      "iteration 800 / 1500: loss 2.092636\n",
      "iteration 900 / 1500: loss 2.112769\n",
      "iteration 1000 / 1500: loss 2.076062\n",
      "iteration 1100 / 1500: loss 2.078413\n",
      "iteration 1200 / 1500: loss 2.157984\n",
      "iteration 1300 / 1500: loss 2.072607\n",
      "iteration 1400 / 1500: loss 2.101195\n",
      "iteration 0 / 1500: loss 1084.476361\n",
      "iteration 100 / 1500: loss 266.703769\n",
      "iteration 200 / 1500: loss 66.936418\n",
      "iteration 300 / 1500: loss 17.934909\n",
      "iteration 400 / 1500: loss 5.981243\n",
      "iteration 500 / 1500: loss 3.058483\n",
      "iteration 600 / 1500: loss 2.348572\n",
      "iteration 700 / 1500: loss 2.138049\n",
      "iteration 800 / 1500: loss 2.121356\n",
      "iteration 900 / 1500: loss 2.110227\n",
      "iteration 1000 / 1500: loss 2.070742\n",
      "iteration 1100 / 1500: loss 2.137974\n",
      "iteration 1200 / 1500: loss 2.143218\n",
      "iteration 1300 / 1500: loss 2.122266\n",
      "iteration 1400 / 1500: loss 2.089879\n",
      "iteration 0 / 1500: loss 1083.728544\n",
      "iteration 100 / 1500: loss 266.470795\n",
      "iteration 200 / 1500: loss 66.859305\n",
      "iteration 300 / 1500: loss 18.011454\n",
      "iteration 400 / 1500: loss 5.962078\n",
      "iteration 500 / 1500: loss 3.020095\n",
      "iteration 600 / 1500: loss 2.340808\n",
      "iteration 700 / 1500: loss 2.156093\n",
      "iteration 800 / 1500: loss 2.148193\n",
      "iteration 900 / 1500: loss 2.150352\n",
      "iteration 1000 / 1500: loss 2.126627\n",
      "iteration 1100 / 1500: loss 2.115728\n",
      "iteration 1200 / 1500: loss 2.098877\n",
      "iteration 1300 / 1500: loss 2.088992\n",
      "iteration 1400 / 1500: loss 2.099277\n",
      "iteration 0 / 1500: loss 1083.276884\n",
      "iteration 100 / 1500: loss 266.622461\n",
      "iteration 200 / 1500: loss 66.751300\n",
      "iteration 300 / 1500: loss 17.941351\n",
      "iteration 400 / 1500: loss 5.938431\n",
      "iteration 500 / 1500: loss 3.052965\n",
      "iteration 600 / 1500: loss 2.318637\n",
      "iteration 700 / 1500: loss 2.142644\n",
      "iteration 800 / 1500: loss 2.102842\n",
      "iteration 900 / 1500: loss 2.147463\n",
      "iteration 1000 / 1500: loss 2.086126\n",
      "iteration 1100 / 1500: loss 2.122223\n",
      "iteration 1200 / 1500: loss 2.124711\n",
      "iteration 1300 / 1500: loss 2.110836\n",
      "iteration 1400 / 1500: loss 2.088888\n",
      "iteration 0 / 1500: loss 1097.821566\n",
      "iteration 100 / 1500: loss 269.901580\n",
      "iteration 200 / 1500: loss 67.609974\n",
      "iteration 300 / 1500: loss 18.099106\n",
      "iteration 400 / 1500: loss 6.048630\n",
      "iteration 500 / 1500: loss 3.060207\n",
      "iteration 600 / 1500: loss 2.328563\n",
      "iteration 700 / 1500: loss 2.117393\n",
      "iteration 800 / 1500: loss 2.140272\n",
      "iteration 900 / 1500: loss 2.136157\n",
      "iteration 1000 / 1500: loss 2.116286\n",
      "iteration 1100 / 1500: loss 2.097759\n",
      "iteration 1200 / 1500: loss 2.131771\n",
      "iteration 1300 / 1500: loss 2.090589\n",
      "iteration 1400 / 1500: loss 2.116430\n",
      "[(0.3147448979591837, 0.31642857142857145), (0.3182397959183674, 0.30428571428571427), (0.30964285714285716, 0.321530612244898), (0.32114795918367345, 0.3220408163265306), (0.3274489795918367, 0.3213265306122449)]\n",
      "validation accuracy: 0.317122\n",
      "l,r: 1e-07 50000.0\n",
      "iteration 0 / 1500: loss 1524.048712\n",
      "iteration 100 / 1500: loss 205.217157\n",
      "iteration 200 / 1500: loss 29.197751\n",
      "iteration 300 / 1500: loss 5.755976\n",
      "iteration 400 / 1500: loss 2.613810\n",
      "iteration 500 / 1500: loss 2.203716\n",
      "iteration 600 / 1500: loss 2.159358\n",
      "iteration 700 / 1500: loss 2.168322\n",
      "iteration 800 / 1500: loss 2.090358\n",
      "iteration 900 / 1500: loss 2.168047\n",
      "iteration 1000 / 1500: loss 2.139445\n",
      "iteration 1100 / 1500: loss 2.143117\n",
      "iteration 1200 / 1500: loss 2.092981\n",
      "iteration 1300 / 1500: loss 2.119629\n",
      "iteration 1400 / 1500: loss 2.132035\n",
      "iteration 0 / 1500: loss 1542.737702\n",
      "iteration 100 / 1500: loss 207.744187\n",
      "iteration 200 / 1500: loss 29.631948\n",
      "iteration 300 / 1500: loss 5.806381\n",
      "iteration 400 / 1500: loss 2.664046\n",
      "iteration 500 / 1500: loss 2.226470\n",
      "iteration 600 / 1500: loss 2.117873\n",
      "iteration 700 / 1500: loss 2.155187\n",
      "iteration 800 / 1500: loss 2.147703\n",
      "iteration 900 / 1500: loss 2.145614\n",
      "iteration 1000 / 1500: loss 2.160316\n",
      "iteration 1100 / 1500: loss 2.111757\n",
      "iteration 1200 / 1500: loss 2.089241\n",
      "iteration 1300 / 1500: loss 2.138019\n",
      "iteration 1400 / 1500: loss 2.136410\n",
      "iteration 0 / 1500: loss 1560.803505\n",
      "iteration 100 / 1500: loss 210.241137\n",
      "iteration 200 / 1500: loss 29.912497\n",
      "iteration 300 / 1500: loss 5.897855\n",
      "iteration 400 / 1500: loss 2.612042\n",
      "iteration 500 / 1500: loss 2.195787\n",
      "iteration 600 / 1500: loss 2.080336\n",
      "iteration 700 / 1500: loss 2.153589\n",
      "iteration 800 / 1500: loss 2.143970\n",
      "iteration 900 / 1500: loss 2.153616\n",
      "iteration 1000 / 1500: loss 2.143801\n",
      "iteration 1100 / 1500: loss 2.139276\n",
      "iteration 1200 / 1500: loss 2.111598\n",
      "iteration 1300 / 1500: loss 2.122763\n",
      "iteration 1400 / 1500: loss 2.162711\n",
      "iteration 0 / 1500: loss 1529.931541\n",
      "iteration 100 / 1500: loss 205.885646\n",
      "iteration 200 / 1500: loss 29.383616\n",
      "iteration 300 / 1500: loss 5.781560\n",
      "iteration 400 / 1500: loss 2.621498\n",
      "iteration 500 / 1500: loss 2.278398\n",
      "iteration 600 / 1500: loss 2.144610\n",
      "iteration 700 / 1500: loss 2.124997\n",
      "iteration 800 / 1500: loss 2.107156\n",
      "iteration 900 / 1500: loss 2.149191\n",
      "iteration 1000 / 1500: loss 2.111518\n",
      "iteration 1100 / 1500: loss 2.121934\n",
      "iteration 1200 / 1500: loss 2.139983\n",
      "iteration 1300 / 1500: loss 2.161936\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1400 / 1500: loss 2.119886\n",
      "iteration 0 / 1500: loss 1554.432540\n",
      "iteration 100 / 1500: loss 209.478560\n",
      "iteration 200 / 1500: loss 29.815760\n",
      "iteration 300 / 1500: loss 5.868269\n",
      "iteration 400 / 1500: loss 2.652462\n",
      "iteration 500 / 1500: loss 2.248784\n",
      "iteration 600 / 1500: loss 2.121433\n",
      "iteration 700 / 1500: loss 2.150659\n",
      "iteration 800 / 1500: loss 2.122467\n",
      "iteration 900 / 1500: loss 2.144143\n",
      "iteration 1000 / 1500: loss 2.147677\n",
      "iteration 1100 / 1500: loss 2.209240\n",
      "iteration 1200 / 1500: loss 2.148120\n",
      "iteration 1300 / 1500: loss 2.124666\n",
      "iteration 1400 / 1500: loss 2.165387\n",
      "[(0.30558673469387754, 0.3062244897959184), (0.3142091836734694, 0.298265306122449), (0.3050765306122449, 0.3137755102040816), (0.3077295918367347, 0.3066326530612245), (0.30196428571428574, 0.29918367346938773)]\n",
      "validation accuracy: 0.304816\n",
      "l,r: 5e-07 25000.0\n",
      "iteration 0 / 1500: loss 777.967794\n",
      "iteration 100 / 1500: loss 6.917416\n",
      "iteration 200 / 1500: loss 2.112842\n",
      "iteration 300 / 1500: loss 2.119916\n",
      "iteration 400 / 1500: loss 2.067412\n",
      "iteration 500 / 1500: loss 2.124737\n",
      "iteration 600 / 1500: loss 2.092786\n",
      "iteration 700 / 1500: loss 2.104422\n",
      "iteration 800 / 1500: loss 2.126695\n",
      "iteration 900 / 1500: loss 2.070173\n",
      "iteration 1000 / 1500: loss 2.102358\n",
      "iteration 1100 / 1500: loss 2.142468\n",
      "iteration 1200 / 1500: loss 2.101723\n",
      "iteration 1300 / 1500: loss 2.110763\n",
      "iteration 1400 / 1500: loss 2.105630\n",
      "iteration 0 / 1500: loss 770.727684\n",
      "iteration 100 / 1500: loss 6.832169\n",
      "iteration 200 / 1500: loss 2.114072\n",
      "iteration 300 / 1500: loss 2.070668\n",
      "iteration 400 / 1500: loss 2.047163\n",
      "iteration 500 / 1500: loss 2.052816\n",
      "iteration 600 / 1500: loss 2.153604\n",
      "iteration 700 / 1500: loss 2.101006\n",
      "iteration 800 / 1500: loss 2.062685\n",
      "iteration 900 / 1500: loss 2.110435\n",
      "iteration 1000 / 1500: loss 2.115757\n",
      "iteration 1100 / 1500: loss 2.109891\n",
      "iteration 1200 / 1500: loss 2.031647\n",
      "iteration 1300 / 1500: loss 2.115876\n",
      "iteration 1400 / 1500: loss 2.087269\n",
      "iteration 0 / 1500: loss 778.113555\n",
      "iteration 100 / 1500: loss 6.960458\n",
      "iteration 200 / 1500: loss 2.113267\n",
      "iteration 300 / 1500: loss 2.079266\n",
      "iteration 400 / 1500: loss 2.118020\n",
      "iteration 500 / 1500: loss 2.034819\n",
      "iteration 600 / 1500: loss 2.093464\n",
      "iteration 700 / 1500: loss 2.132302\n",
      "iteration 800 / 1500: loss 2.069782\n",
      "iteration 900 / 1500: loss 2.075503\n",
      "iteration 1000 / 1500: loss 2.098688\n",
      "iteration 1100 / 1500: loss 2.144023\n",
      "iteration 1200 / 1500: loss 2.089664\n",
      "iteration 1300 / 1500: loss 2.098478\n",
      "iteration 1400 / 1500: loss 2.134953\n",
      "iteration 0 / 1500: loss 779.768736\n",
      "iteration 100 / 1500: loss 7.001075\n",
      "iteration 200 / 1500: loss 2.123446\n",
      "iteration 300 / 1500: loss 2.072871\n",
      "iteration 400 / 1500: loss 2.104433\n",
      "iteration 500 / 1500: loss 2.071454\n",
      "iteration 600 / 1500: loss 2.110064\n",
      "iteration 700 / 1500: loss 2.133114\n",
      "iteration 800 / 1500: loss 2.082293\n",
      "iteration 900 / 1500: loss 2.018246\n",
      "iteration 1000 / 1500: loss 2.082996\n",
      "iteration 1100 / 1500: loss 2.081430\n",
      "iteration 1200 / 1500: loss 2.082791\n",
      "iteration 1300 / 1500: loss 2.065881\n",
      "iteration 1400 / 1500: loss 2.110083\n",
      "iteration 0 / 1500: loss 779.139248\n",
      "iteration 100 / 1500: loss 6.926551\n",
      "iteration 200 / 1500: loss 2.120036\n",
      "iteration 300 / 1500: loss 2.106611\n",
      "iteration 400 / 1500: loss 2.091850\n",
      "iteration 500 / 1500: loss 2.134432\n",
      "iteration 600 / 1500: loss 2.035937\n",
      "iteration 700 / 1500: loss 2.089482\n",
      "iteration 800 / 1500: loss 2.034700\n",
      "iteration 900 / 1500: loss 2.073614\n",
      "iteration 1000 / 1500: loss 2.072702\n",
      "iteration 1100 / 1500: loss 2.165556\n",
      "iteration 1200 / 1500: loss 2.028283\n",
      "iteration 1300 / 1500: loss 2.119785\n",
      "iteration 1400 / 1500: loss 2.114569\n",
      "[(0.33290816326530615, 0.3327551020408163), (0.3262244897959184, 0.31418367346938775), (0.33512755102040814, 0.33877551020408164), (0.3353061224489796, 0.33591836734693875), (0.3251530612244898, 0.32479591836734695)]\n",
      "validation accuracy: 0.329286\n",
      "l,r: 5e-07 35000.0\n",
      "iteration 0 / 1500: loss 1084.189722\n",
      "iteration 100 / 1500: loss 2.982981\n",
      "iteration 200 / 1500: loss 2.134405\n",
      "iteration 300 / 1500: loss 2.133192\n",
      "iteration 400 / 1500: loss 2.104017\n",
      "iteration 500 / 1500: loss 2.117647\n",
      "iteration 600 / 1500: loss 2.100647\n",
      "iteration 700 / 1500: loss 2.130997\n",
      "iteration 800 / 1500: loss 2.129089\n",
      "iteration 900 / 1500: loss 2.105735\n",
      "iteration 1000 / 1500: loss 2.164274\n",
      "iteration 1100 / 1500: loss 2.146868\n",
      "iteration 1200 / 1500: loss 2.133336\n",
      "iteration 1300 / 1500: loss 2.089680\n",
      "iteration 1400 / 1500: loss 2.148078\n",
      "iteration 0 / 1500: loss 1069.871615\n",
      "iteration 100 / 1500: loss 3.012542\n",
      "iteration 200 / 1500: loss 2.093462\n",
      "iteration 300 / 1500: loss 2.105228\n",
      "iteration 400 / 1500: loss 2.093312\n",
      "iteration 500 / 1500: loss 2.101524\n",
      "iteration 600 / 1500: loss 2.168272\n",
      "iteration 700 / 1500: loss 2.165012\n",
      "iteration 800 / 1500: loss 2.104198\n",
      "iteration 900 / 1500: loss 2.124343\n",
      "iteration 1000 / 1500: loss 2.178982\n",
      "iteration 1100 / 1500: loss 2.108019\n",
      "iteration 1200 / 1500: loss 2.118058\n",
      "iteration 1300 / 1500: loss 2.122603\n",
      "iteration 1400 / 1500: loss 2.120384\n",
      "iteration 0 / 1500: loss 1068.501859\n",
      "iteration 100 / 1500: loss 2.933727\n",
      "iteration 200 / 1500: loss 2.098035\n",
      "iteration 300 / 1500: loss 2.105577\n",
      "iteration 400 / 1500: loss 2.103710\n",
      "iteration 500 / 1500: loss 2.115405\n",
      "iteration 600 / 1500: loss 2.123528\n",
      "iteration 700 / 1500: loss 2.136313\n",
      "iteration 800 / 1500: loss 2.155900\n",
      "iteration 900 / 1500: loss 2.107393\n",
      "iteration 1000 / 1500: loss 2.097236\n",
      "iteration 1100 / 1500: loss 2.128444\n",
      "iteration 1200 / 1500: loss 2.154405\n",
      "iteration 1300 / 1500: loss 2.104835\n",
      "iteration 1400 / 1500: loss 2.153198\n",
      "iteration 0 / 1500: loss 1077.082897\n",
      "iteration 100 / 1500: loss 2.972988\n",
      "iteration 200 / 1500: loss 2.084410\n",
      "iteration 300 / 1500: loss 2.117095\n",
      "iteration 400 / 1500: loss 2.074843\n",
      "iteration 500 / 1500: loss 2.139003\n",
      "iteration 600 / 1500: loss 2.095142\n",
      "iteration 700 / 1500: loss 2.088432\n",
      "iteration 800 / 1500: loss 2.167138\n",
      "iteration 900 / 1500: loss 2.136146\n",
      "iteration 1000 / 1500: loss 2.086024\n",
      "iteration 1100 / 1500: loss 2.056025\n",
      "iteration 1200 / 1500: loss 2.120549\n",
      "iteration 1300 / 1500: loss 2.192263\n",
      "iteration 1400 / 1500: loss 2.100020\n",
      "iteration 0 / 1500: loss 1075.467994\n",
      "iteration 100 / 1500: loss 2.928209\n",
      "iteration 200 / 1500: loss 2.166212\n",
      "iteration 300 / 1500: loss 2.099296\n",
      "iteration 400 / 1500: loss 2.087397\n",
      "iteration 500 / 1500: loss 2.123808\n",
      "iteration 600 / 1500: loss 2.118220\n",
      "iteration 700 / 1500: loss 2.106055\n",
      "iteration 800 / 1500: loss 2.143408\n",
      "iteration 900 / 1500: loss 2.125947\n",
      "iteration 1000 / 1500: loss 2.136035\n",
      "iteration 1100 / 1500: loss 2.152517\n",
      "iteration 1200 / 1500: loss 2.156803\n",
      "iteration 1300 / 1500: loss 2.132942\n",
      "iteration 1400 / 1500: loss 2.115416\n",
      "[(0.3053061224489796, 0.30214285714285716), (0.2951530612244898, 0.2841836734693878), (0.31395408163265304, 0.32081632653061226), (0.3138520408163265, 0.3163265306122449), (0.31323979591836737, 0.30704081632653063)]\n",
      "validation accuracy: 0.306102\n",
      "l,r: 5e-07 50000.0\n",
      "iteration 0 / 1500: loss 1528.781465\n",
      "iteration 100 / 1500: loss 2.181969\n",
      "iteration 200 / 1500: loss 2.129152\n",
      "iteration 300 / 1500: loss 2.106141\n",
      "iteration 400 / 1500: loss 2.110745\n",
      "iteration 500 / 1500: loss 2.093897\n",
      "iteration 600 / 1500: loss 2.158736\n",
      "iteration 700 / 1500: loss 2.161106\n",
      "iteration 800 / 1500: loss 2.126249\n",
      "iteration 900 / 1500: loss 2.131107\n",
      "iteration 1000 / 1500: loss 2.188893\n",
      "iteration 1100 / 1500: loss 2.095939\n",
      "iteration 1200 / 1500: loss 2.160676\n",
      "iteration 1300 / 1500: loss 2.200549\n",
      "iteration 1400 / 1500: loss 2.099760\n",
      "iteration 0 / 1500: loss 1546.148179\n",
      "iteration 100 / 1500: loss 2.172670\n",
      "iteration 200 / 1500: loss 2.131935\n",
      "iteration 300 / 1500: loss 2.124412\n",
      "iteration 400 / 1500: loss 2.197936\n",
      "iteration 500 / 1500: loss 2.175749\n",
      "iteration 600 / 1500: loss 2.149016\n",
      "iteration 700 / 1500: loss 2.141349\n",
      "iteration 800 / 1500: loss 2.124089\n",
      "iteration 900 / 1500: loss 2.121601\n",
      "iteration 1000 / 1500: loss 2.129200\n",
      "iteration 1100 / 1500: loss 2.167240\n",
      "iteration 1200 / 1500: loss 2.165302\n",
      "iteration 1300 / 1500: loss 2.084985\n",
      "iteration 1400 / 1500: loss 2.126390\n",
      "iteration 0 / 1500: loss 1542.482438\n",
      "iteration 100 / 1500: loss 2.234472\n",
      "iteration 200 / 1500: loss 2.169155\n",
      "iteration 300 / 1500: loss 2.138221\n",
      "iteration 400 / 1500: loss 2.172251\n",
      "iteration 500 / 1500: loss 2.154223\n",
      "iteration 600 / 1500: loss 2.089119\n",
      "iteration 700 / 1500: loss 2.097970\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 800 / 1500: loss 2.167016\n",
      "iteration 900 / 1500: loss 2.168746\n",
      "iteration 1000 / 1500: loss 2.162821\n",
      "iteration 1100 / 1500: loss 2.123857\n",
      "iteration 1200 / 1500: loss 2.176642\n",
      "iteration 1300 / 1500: loss 2.147909\n",
      "iteration 1400 / 1500: loss 2.117126\n",
      "iteration 0 / 1500: loss 1535.642338\n",
      "iteration 100 / 1500: loss 2.225707\n",
      "iteration 200 / 1500: loss 2.113363\n",
      "iteration 300 / 1500: loss 2.154582\n",
      "iteration 400 / 1500: loss 2.148832\n",
      "iteration 500 / 1500: loss 2.152833\n",
      "iteration 600 / 1500: loss 2.100314\n",
      "iteration 700 / 1500: loss 2.147478\n",
      "iteration 800 / 1500: loss 2.169570\n",
      "iteration 900 / 1500: loss 2.137174\n",
      "iteration 1000 / 1500: loss 2.125000\n",
      "iteration 1100 / 1500: loss 2.114884\n",
      "iteration 1200 / 1500: loss 2.164374\n",
      "iteration 1300 / 1500: loss 2.135679\n",
      "iteration 1400 / 1500: loss 2.127622\n",
      "iteration 0 / 1500: loss 1529.778339\n",
      "iteration 100 / 1500: loss 2.210718\n",
      "iteration 200 / 1500: loss 2.181791\n",
      "iteration 300 / 1500: loss 2.208864\n",
      "iteration 400 / 1500: loss 2.181882\n",
      "iteration 500 / 1500: loss 2.149204\n",
      "iteration 600 / 1500: loss 2.161308\n",
      "iteration 700 / 1500: loss 2.177108\n",
      "iteration 800 / 1500: loss 2.135379\n",
      "iteration 900 / 1500: loss 2.185289\n",
      "iteration 1000 / 1500: loss 2.199603\n",
      "iteration 1100 / 1500: loss 2.142141\n",
      "iteration 1200 / 1500: loss 2.160387\n",
      "iteration 1300 / 1500: loss 2.155714\n",
      "iteration 1400 / 1500: loss 2.121692\n",
      "[(0.2980357142857143, 0.2969387755102041), (0.3106122448979592, 0.2969387755102041), (0.2888265306122449, 0.29836734693877554), (0.3040816326530612, 0.303469387755102), (0.3006122448979592, 0.2978571428571429)]\n",
      "validation accuracy: 0.298714\n",
      "l,r: 3e-07 25000.0\n",
      "iteration 0 / 1500: loss 764.245700\n",
      "iteration 100 / 1500: loss 38.803033\n",
      "iteration 200 / 1500: loss 3.867762\n",
      "iteration 300 / 1500: loss 2.139072\n",
      "iteration 400 / 1500: loss 1.984202\n",
      "iteration 500 / 1500: loss 2.060627\n",
      "iteration 600 / 1500: loss 2.098899\n",
      "iteration 700 / 1500: loss 2.095842\n",
      "iteration 800 / 1500: loss 2.067789\n",
      "iteration 900 / 1500: loss 2.067980\n",
      "iteration 1000 / 1500: loss 2.056700\n",
      "iteration 1100 / 1500: loss 2.098196\n",
      "iteration 1200 / 1500: loss 2.033106\n",
      "iteration 1300 / 1500: loss 2.124372\n",
      "iteration 1400 / 1500: loss 2.089675\n",
      "iteration 0 / 1500: loss 780.538771\n",
      "iteration 100 / 1500: loss 39.569030\n",
      "iteration 200 / 1500: loss 3.834575\n",
      "iteration 300 / 1500: loss 2.180419\n",
      "iteration 400 / 1500: loss 2.050829\n",
      "iteration 500 / 1500: loss 2.016559\n",
      "iteration 600 / 1500: loss 2.055909\n",
      "iteration 700 / 1500: loss 2.053437\n",
      "iteration 800 / 1500: loss 2.122094\n",
      "iteration 900 / 1500: loss 2.088521\n",
      "iteration 1000 / 1500: loss 2.095840\n",
      "iteration 1100 / 1500: loss 2.045347\n",
      "iteration 1200 / 1500: loss 2.104196\n",
      "iteration 1300 / 1500: loss 2.006534\n",
      "iteration 1400 / 1500: loss 2.146010\n",
      "iteration 0 / 1500: loss 769.057025\n",
      "iteration 100 / 1500: loss 39.084041\n",
      "iteration 200 / 1500: loss 3.836728\n",
      "iteration 300 / 1500: loss 2.173151\n",
      "iteration 400 / 1500: loss 2.107470\n",
      "iteration 500 / 1500: loss 2.083229\n",
      "iteration 600 / 1500: loss 2.011173\n",
      "iteration 700 / 1500: loss 2.086216\n",
      "iteration 800 / 1500: loss 2.057620\n",
      "iteration 900 / 1500: loss 2.119111\n",
      "iteration 1000 / 1500: loss 2.008375\n",
      "iteration 1100 / 1500: loss 2.097375\n",
      "iteration 1200 / 1500: loss 2.080028\n",
      "iteration 1300 / 1500: loss 2.053236\n",
      "iteration 1400 / 1500: loss 2.057514\n",
      "iteration 0 / 1500: loss 772.501735\n",
      "iteration 100 / 1500: loss 39.144642\n",
      "iteration 200 / 1500: loss 3.931372\n",
      "iteration 300 / 1500: loss 2.233231\n",
      "iteration 400 / 1500: loss 2.122601\n",
      "iteration 500 / 1500: loss 2.152290\n",
      "iteration 600 / 1500: loss 2.012062\n",
      "iteration 700 / 1500: loss 2.094227\n",
      "iteration 800 / 1500: loss 2.055391\n",
      "iteration 900 / 1500: loss 2.096124\n",
      "iteration 1000 / 1500: loss 2.085721\n",
      "iteration 1100 / 1500: loss 2.086340\n",
      "iteration 1200 / 1500: loss 2.103093\n",
      "iteration 1300 / 1500: loss 2.031928\n",
      "iteration 1400 / 1500: loss 2.089372\n",
      "iteration 0 / 1500: loss 767.299749\n",
      "iteration 100 / 1500: loss 38.909832\n",
      "iteration 200 / 1500: loss 3.865735\n",
      "iteration 300 / 1500: loss 2.147352\n",
      "iteration 400 / 1500: loss 2.079872\n",
      "iteration 500 / 1500: loss 2.114248\n",
      "iteration 600 / 1500: loss 2.015453\n",
      "iteration 700 / 1500: loss 2.058188\n",
      "iteration 800 / 1500: loss 2.053708\n",
      "iteration 900 / 1500: loss 2.043712\n",
      "iteration 1000 / 1500: loss 2.087888\n",
      "iteration 1100 / 1500: loss 2.059129\n",
      "iteration 1200 / 1500: loss 2.119837\n",
      "iteration 1300 / 1500: loss 2.060359\n",
      "iteration 1400 / 1500: loss 2.111055\n",
      "[(0.329234693877551, 0.32693877551020406), (0.32002551020408165, 0.3081632653061224), (0.3292091836734694, 0.3346938775510204), (0.32913265306122447, 0.32857142857142857), (0.33364795918367346, 0.3279591836734694)]\n",
      "validation accuracy: 0.325265\n",
      "l,r: 3e-07 35000.0\n",
      "iteration 0 / 1500: loss 1087.994930\n",
      "iteration 100 / 1500: loss 17.541930\n",
      "iteration 200 / 1500: loss 2.281687\n",
      "iteration 300 / 1500: loss 2.141175\n",
      "iteration 400 / 1500: loss 2.127517\n",
      "iteration 500 / 1500: loss 2.159318\n",
      "iteration 600 / 1500: loss 2.084302\n",
      "iteration 700 / 1500: loss 2.111488\n",
      "iteration 800 / 1500: loss 2.143875\n",
      "iteration 900 / 1500: loss 2.108433\n",
      "iteration 1000 / 1500: loss 2.115751\n",
      "iteration 1100 / 1500: loss 2.082876\n",
      "iteration 1200 / 1500: loss 2.142696\n",
      "iteration 1300 / 1500: loss 2.083290\n",
      "iteration 1400 / 1500: loss 2.102731\n",
      "iteration 0 / 1500: loss 1082.948965\n",
      "iteration 100 / 1500: loss 17.517025\n",
      "iteration 200 / 1500: loss 2.348340\n",
      "iteration 300 / 1500: loss 2.118156\n",
      "iteration 400 / 1500: loss 2.113587\n",
      "iteration 500 / 1500: loss 2.056961\n",
      "iteration 600 / 1500: loss 2.149899\n",
      "iteration 700 / 1500: loss 2.084995\n",
      "iteration 800 / 1500: loss 2.093085\n",
      "iteration 900 / 1500: loss 2.127100\n",
      "iteration 1000 / 1500: loss 2.108516\n",
      "iteration 1100 / 1500: loss 2.102050\n",
      "iteration 1200 / 1500: loss 2.084129\n",
      "iteration 1300 / 1500: loss 2.085169\n",
      "iteration 1400 / 1500: loss 2.122198\n",
      "iteration 0 / 1500: loss 1086.670003\n",
      "iteration 100 / 1500: loss 17.529391\n",
      "iteration 200 / 1500: loss 2.365319\n",
      "iteration 300 / 1500: loss 2.139988\n",
      "iteration 400 / 1500: loss 2.119539\n",
      "iteration 500 / 1500: loss 2.183509\n",
      "iteration 600 / 1500: loss 2.049665\n",
      "iteration 700 / 1500: loss 2.099478\n",
      "iteration 800 / 1500: loss 2.043854\n",
      "iteration 900 / 1500: loss 2.127563\n",
      "iteration 1000 / 1500: loss 2.120174\n",
      "iteration 1100 / 1500: loss 2.104915\n",
      "iteration 1200 / 1500: loss 2.083653\n",
      "iteration 1300 / 1500: loss 2.144914\n",
      "iteration 1400 / 1500: loss 2.097676\n",
      "iteration 0 / 1500: loss 1074.558885\n",
      "iteration 100 / 1500: loss 17.299545\n",
      "iteration 200 / 1500: loss 2.324005\n",
      "iteration 300 / 1500: loss 2.111471\n",
      "iteration 400 / 1500: loss 2.162235\n",
      "iteration 500 / 1500: loss 2.111440\n",
      "iteration 600 / 1500: loss 2.103515\n",
      "iteration 700 / 1500: loss 2.075510\n",
      "iteration 800 / 1500: loss 2.125913\n",
      "iteration 900 / 1500: loss 2.091864\n",
      "iteration 1000 / 1500: loss 2.187241\n",
      "iteration 1100 / 1500: loss 2.147069\n",
      "iteration 1200 / 1500: loss 2.117253\n",
      "iteration 1300 / 1500: loss 2.124347\n",
      "iteration 1400 / 1500: loss 2.098304\n",
      "iteration 0 / 1500: loss 1068.550890\n",
      "iteration 100 / 1500: loss 17.258845\n",
      "iteration 200 / 1500: loss 2.296994\n",
      "iteration 300 / 1500: loss 2.130089\n",
      "iteration 400 / 1500: loss 2.100480\n",
      "iteration 500 / 1500: loss 2.130096\n",
      "iteration 600 / 1500: loss 2.087102\n",
      "iteration 700 / 1500: loss 2.072526\n",
      "iteration 800 / 1500: loss 2.079753\n",
      "iteration 900 / 1500: loss 2.146667\n",
      "iteration 1000 / 1500: loss 2.106464\n",
      "iteration 1100 / 1500: loss 2.104812\n",
      "iteration 1200 / 1500: loss 2.087319\n",
      "iteration 1300 / 1500: loss 2.098461\n",
      "iteration 1400 / 1500: loss 2.179273\n",
      "[(0.3068112244897959, 0.30653061224489797), (0.31339285714285714, 0.2993877551020408), (0.3148724489795918, 0.32255102040816325), (0.3253826530612245, 0.3248979591836735), (0.3074234693877551, 0.3036734693877551)]\n",
      "validation accuracy: 0.311408\n",
      "l,r: 3e-07 50000.0\n",
      "iteration 0 / 1500: loss 1560.925231\n",
      "iteration 100 / 1500: loss 5.644440\n",
      "iteration 200 / 1500: loss 2.134149\n",
      "iteration 300 / 1500: loss 2.150220\n",
      "iteration 400 / 1500: loss 2.158609\n",
      "iteration 500 / 1500: loss 2.188046\n",
      "iteration 600 / 1500: loss 2.167779\n",
      "iteration 700 / 1500: loss 2.159522\n",
      "iteration 800 / 1500: loss 2.165640\n",
      "iteration 900 / 1500: loss 2.159278\n",
      "iteration 1000 / 1500: loss 2.137890\n",
      "iteration 1100 / 1500: loss 2.154653\n",
      "iteration 1200 / 1500: loss 2.086597\n",
      "iteration 1300 / 1500: loss 2.115001\n",
      "iteration 1400 / 1500: loss 2.106668\n",
      "iteration 0 / 1500: loss 1564.614876\n",
      "iteration 100 / 1500: loss 5.646777\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 200 / 1500: loss 2.135936\n",
      "iteration 300 / 1500: loss 2.138616\n",
      "iteration 400 / 1500: loss 2.143554\n",
      "iteration 500 / 1500: loss 2.119138\n",
      "iteration 600 / 1500: loss 2.112842\n",
      "iteration 700 / 1500: loss 2.147769\n",
      "iteration 800 / 1500: loss 2.167199\n",
      "iteration 900 / 1500: loss 2.165265\n",
      "iteration 1000 / 1500: loss 2.116583\n",
      "iteration 1100 / 1500: loss 2.147084\n",
      "iteration 1200 / 1500: loss 2.083232\n",
      "iteration 1300 / 1500: loss 2.136461\n",
      "iteration 1400 / 1500: loss 2.115959\n",
      "iteration 0 / 1500: loss 1546.315291\n",
      "iteration 100 / 1500: loss 5.609535\n",
      "iteration 200 / 1500: loss 2.156543\n",
      "iteration 300 / 1500: loss 2.159341\n",
      "iteration 400 / 1500: loss 2.124564\n",
      "iteration 500 / 1500: loss 2.125280\n",
      "iteration 600 / 1500: loss 2.152792\n",
      "iteration 700 / 1500: loss 2.171049\n",
      "iteration 800 / 1500: loss 2.072351\n",
      "iteration 900 / 1500: loss 2.119657\n",
      "iteration 1000 / 1500: loss 2.144601\n",
      "iteration 1100 / 1500: loss 2.143656\n",
      "iteration 1200 / 1500: loss 2.176530\n",
      "iteration 1300 / 1500: loss 2.128122\n",
      "iteration 1400 / 1500: loss 2.179625\n",
      "iteration 0 / 1500: loss 1547.389661\n",
      "iteration 100 / 1500: loss 5.581614\n",
      "iteration 200 / 1500: loss 2.201073\n",
      "iteration 300 / 1500: loss 2.134837\n",
      "iteration 400 / 1500: loss 2.185498\n",
      "iteration 500 / 1500: loss 2.122573\n",
      "iteration 600 / 1500: loss 2.094614\n",
      "iteration 700 / 1500: loss 2.141303\n",
      "iteration 800 / 1500: loss 2.171318\n",
      "iteration 900 / 1500: loss 2.193194\n",
      "iteration 1000 / 1500: loss 2.153773\n",
      "iteration 1100 / 1500: loss 2.172795\n",
      "iteration 1200 / 1500: loss 2.130646\n",
      "iteration 1300 / 1500: loss 2.167306\n",
      "iteration 1400 / 1500: loss 2.129231\n",
      "iteration 0 / 1500: loss 1542.558437\n",
      "iteration 100 / 1500: loss 5.590965\n",
      "iteration 200 / 1500: loss 2.135937\n",
      "iteration 300 / 1500: loss 2.166561\n",
      "iteration 400 / 1500: loss 2.157133\n",
      "iteration 500 / 1500: loss 2.161973\n",
      "iteration 600 / 1500: loss 2.117090\n",
      "iteration 700 / 1500: loss 2.163770\n",
      "iteration 800 / 1500: loss 2.126249\n",
      "iteration 900 / 1500: loss 2.129559\n",
      "iteration 1000 / 1500: loss 2.129080\n",
      "iteration 1100 / 1500: loss 2.120674\n",
      "iteration 1200 / 1500: loss 2.145265\n",
      "iteration 1300 / 1500: loss 2.165295\n",
      "iteration 1400 / 1500: loss 2.155533\n",
      "[(0.29482142857142857, 0.29806122448979594), (0.29709183673469386, 0.28551020408163263), (0.2951530612244898, 0.30326530612244895), (0.31645408163265304, 0.31612244897959185), (0.3092091836734694, 0.3036734693877551)]\n",
      "validation accuracy: 0.301327\n",
      "lr 1.000000e-07 reg 2.500000e+04 train accuracy: 0.330816 val accuracy: 0.328857\n",
      "lr 1.000000e-07 reg 3.500000e+04 train accuracy: 0.318245 val accuracy: 0.317122\n",
      "lr 1.000000e-07 reg 5.000000e+04 train accuracy: 0.306913 val accuracy: 0.304816\n",
      "lr 3.000000e-07 reg 2.500000e+04 train accuracy: 0.328250 val accuracy: 0.325265\n",
      "lr 3.000000e-07 reg 3.500000e+04 train accuracy: 0.313577 val accuracy: 0.311408\n",
      "lr 3.000000e-07 reg 5.000000e+04 train accuracy: 0.302546 val accuracy: 0.301327\n",
      "lr 5.000000e-07 reg 2.500000e+04 train accuracy: 0.330944 val accuracy: 0.329286\n",
      "lr 5.000000e-07 reg 3.500000e+04 train accuracy: 0.308301 val accuracy: 0.306102\n",
      "lr 5.000000e-07 reg 5.000000e+04 train accuracy: 0.300434 val accuracy: 0.298714\n",
      "best validation accuracy achieved during cross-validation: 0.329286\n"
     ]
    }
   ],
   "source": [
    "# Use the validation set to tune hyperparameters (regularization strength and\n",
    "# learning rate). You should experiment with different ranges for the learning\n",
    "# rates and regularization strengths; if you are careful you should be able to\n",
    "# get a classification accuracy of over 0.35 on the validation set.\n",
    "from cs231n.classifiers import Softmax\n",
    "results = {}\n",
    "best_val = -1\n",
    "best_softmax = None\n",
    "learning_rates = [1e-7, 5e-7, 3e-7]\n",
    "regularization_strengths = [2.5e4, 3.5e4, 5e4]\n",
    "\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Use the validation set to set the learning rate and regularization strength. #\n",
    "# This should be identical to the validation that you did for the SVM; save    #\n",
    "# the best trained softmax classifer in best_softmax.                          #\n",
    "################################################################################\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "num_training = X_train.shape[0]\n",
    "\n",
    "#divide training into folds\n",
    "folds = []\n",
    "folds_label = []\n",
    "num_folds = 5\n",
    "num_data_per_fold = num_training / num_folds\n",
    "\n",
    "for i in range(num_folds):\n",
    "    folds.append(X_train[int(i*num_data_per_fold):int((i+1)*num_data_per_fold),:])\n",
    "    folds_label.append(y_train[int(i*num_data_per_fold):int((i+1)*num_data_per_fold)])\n",
    "    \n",
    "parameters = [ (l,r) for l in learning_rates for r in regularization_strengths ]\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "for (l,r) in parameters:\n",
    "    \n",
    "    accuracies = []\n",
    "    \n",
    "    print(\"l,r:\", l,r)\n",
    "    \n",
    "    classifier_sm = Softmax()\n",
    "    \n",
    "    for i in range(num_folds):\n",
    "        \n",
    "        classifier_sm = Softmax()\n",
    "        \n",
    "        training_set = [folds[j] for j in range(num_folds) if j != i]\n",
    "        training_label = [folds_label[j] for j in range(num_folds) if j != i]\n",
    "        \n",
    "        t_set = np.concatenate(training_set)\n",
    "        t_label_set = np.concatenate(training_label)\n",
    "        v_set = folds[i]\n",
    "        v_label = folds_label[i]\n",
    "        \n",
    "        classifier_sm.train(t_set, t_label_set, l, r, num_iters=1500, verbose=True)\n",
    "        \n",
    "        pred_train = classifier_sm.predict(t_set)\n",
    "        pred_validate = classifier_sm.predict(v_set)\n",
    "        \n",
    "        accuracy_train = np.mean(pred_train == t_label_set)\n",
    "        accuracy_validation = np.mean(pred_validate == v_label)\n",
    "        \n",
    "        accuracies.append((accuracy_train,accuracy_validation))\n",
    "\n",
    "    print(accuracies)\n",
    "\n",
    "    accuracy = reduce(lambda a,b: (a[0]+b[0],a[1]+b[1]), accuracies, (0,0))\n",
    "    \n",
    "    f = 1.0/len(accuracies)\n",
    "\n",
    "    results[l,r] = (accuracy[0]*f, accuracy[1]*f)\n",
    "    if accuracy[1]*f > best_val:\n",
    "        best_softmax = classifier_sm\n",
    "    best_val = max(best_val,accuracy[1]*f)\n",
    "    print('validation accuracy: %f' % (accuracy[1]*f))    \n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    \n",
    "# Print out results.\n",
    "for lr, reg in sorted(results):\n",
    "    train_accuracy, val_accuracy = results[(lr, reg)]\n",
    "    print('lr %e reg %e train accuracy: %f val accuracy: %f' % (\n",
    "                lr, reg, train_accuracy, val_accuracy))\n",
    "    \n",
    "print('best validation accuracy achieved during cross-validation: %f' % best_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax on raw pixels final test set accuracy: 0.340000\n"
     ]
    }
   ],
   "source": [
    "# evaluate on test set\n",
    "# Evaluate the best softmax on test set\n",
    "y_test_pred = best_softmax.predict(X_test)\n",
    "test_accuracy = np.mean(y_test == y_test_pred)\n",
    "print('softmax on raw pixels final test set accuracy: %f' % (test_accuracy, ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "pdf-inline"
    ]
   },
   "source": [
    "**Inline Question 2** - *True or False*\n",
    "\n",
    "Suppose the overall training loss is defined as the sum of the per-datapoint loss over all training examples. It is possible to add a new datapoint to a training set that would leave the SVM loss unchanged, but this is not the case with the Softmax classifier loss.\n",
    "\n",
    "$\\color{blue}{\\textit Your Answer:}$\n",
    "T\n",
    "\n",
    "$\\color{blue}{\\textit Your Explanation:}$\n",
    "The loss function involves fractional terms in which insertion of new samples would perturb numerator and denominator equally thus making the fractional term not equal to before\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the learned weights for each class\n",
    "w = best_softmax.W[:-1,:] # strip out the bias\n",
    "w = w.reshape(32, 32, 3, 10)\n",
    "\n",
    "w_min, w_max = np.min(w), np.max(w)\n",
    "\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    \n",
    "    # Rescale the weights to be between 0 and 255\n",
    "    wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)\n",
    "    plt.imshow(wimg.astype('uint8'))\n",
    "    plt.axis('off')\n",
    "    plt.title(classes[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
